{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization\n"
      ],
      "metadata": {
        "id": "aa4-wD5_h4Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GeZrvBgjADg",
        "outputId": "1745c0bf-0a53-40fa-9109-f1a5b291c4ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoXrByZFh1lo",
        "outputId": "332ce75b-4cae-4552-f293-239657e3505c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "sent_tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rluIN7tpjMe-",
        "outputId": "aa53ab1e-98ae-4d06-8d54-1b9e8bc4249d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regional language filtration"
      ],
      "metadata": {
        "id": "YjDhefquj7MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect nltk\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIjKX03MkDb-",
        "outputId": "22179a53-9729-4cad-ca68-5c73c6afd6a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m757.8/981.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=46940592cf7622ba19984874436607abb248368d951dc385d68ad61996552d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\n",
        "नमस्कार सर्वांना. गिक्सफॉरगिक्समध्ये आपले स्वागत आहे. तुम्ही NLP लेख वाचन करत आहात.\n",
        "\"\"\"\n",
        "\n",
        "def filter_marathi_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    marathi_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        try:\n",
        "            if detect(sentence) == 'mr':  # 'mr' is the language code for Marathi\n",
        "                marathi_sentences.append(sentence)\n",
        "        except LangDetectException:\n",
        "            continue\n",
        "\n",
        "    return marathi_sentences\n",
        "\n",
        "# Filter Marathi sentences from the text\n",
        "marathi_sentences = filter_marathi_sentences(text)\n",
        "\n",
        "# Print the Marathi sentences\n",
        "for sentence in marathi_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNBNERrKj-8t",
        "outputId": "6e4b3012-5aab-4e8d-9e24-8b536fbf69cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "नमस्कार सर्वांना.\n",
            "गिक्सफॉरगिक्समध्ये आपले स्वागत आहे.\n",
            "तुम्ही NLP लेख वाचन करत आहात.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop word filtration"
      ],
      "metadata": {
        "id": "nkkgBiutlHGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fkNdEE5lLeX",
        "outputId": "4bcd2d64-b54b-4adc-ae86-3caceaeac92a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "\t\t\t\tshowing off the stop words filtration.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether\n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC12xOYllO0O",
        "outputId": "50dd7a5d-c4ec-4e8f-a823-60710aa21537"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation filtration"
      ],
      "metadata": {
        "id": "ubfX8Ok9mshA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing string\n",
        "test_str = \"Gfg, is best : for ! Geeks ;\"\n",
        "\n",
        "# printing original string\n",
        "print(\"The original string is : \" + test_str)\n",
        "\n",
        "# initializing punctuations string\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "# Removing punctuations in string\n",
        "# Using loop + punctuation string\n",
        "for ele in test_str:\n",
        "\tif ele in punc:\n",
        "\t\ttest_str = test_str.replace(ele, \"\")\n",
        "\n",
        "# printing result\n",
        "print(\"The string after punctuation filter : \" + test_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJKu2Svfm0OO",
        "outputId": "5e9410ae-2c7f-41e7-ae46-2118833abaaa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original string is : Gfg, is best : for ! Geeks ;\n",
            "The string after punctuation filter : Gfg is best  for  Geeks \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation"
      ],
      "metadata": {
        "id": "DJRpcHnPrJz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def validate_name(name):\n",
        "    \"\"\"Validate if the name contains only alphabets and spaces.\"\"\"\n",
        "    if re.match(r\"^[A-Za-z\\s]+$\", name):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def validate_phone_number(phone_number):\n",
        "    \"\"\"Validate if the phone number is in the format (123) 456-7890 or 123-456-7890.\"\"\"\n",
        "    if re.match(r\"^(\\(\\d{3}\\)\\s|\\d{3}-)\\d{3}-\\d{4}$\", phone_number):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"Validate if the email address is in a valid format.\"\"\"\n",
        "    if re.match(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\", email):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Test the validation functions\n",
        "def test_validations():\n",
        "    names = [\"John Doe\",\"Alice!\"]\n",
        "    phone_numbers = [\"(123) 456-7890\", \"12345\"]\n",
        "    emails = [\"example@example.com\", \"user@domain\"]\n",
        "\n",
        "    print(\"Name Validation:\")\n",
        "    for name in names:\n",
        "        print(f\"{name}: {'Valid' if validate_name(name) else 'Invalid'}\")\n",
        "\n",
        "    print(\"\\nPhone Number Validation:\")\n",
        "    for phone in phone_numbers:\n",
        "        print(f\"{phone}: {'Valid' if validate_phone_number(phone) else 'Invalid'}\")\n",
        "\n",
        "    print(\"\\nEmail Validation:\")\n",
        "    for email in emails:\n",
        "        print(f\"{email}: {'Valid' if validate_email(email) else 'Invalid'}\")\n",
        "\n",
        "# Run the test\n",
        "test_validations()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJAjfOdVrZmc",
        "outputId": "ef28ca15-2772-464b-aafc-cad756cdaec1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name Validation:\n",
            "John Doe: Valid\n",
            "Alice!: Invalid\n",
            "\n",
            "Phone Number Validation:\n",
            "(123) 456-7890: Valid\n",
            "12345: Invalid\n",
            "\n",
            "Email Validation:\n",
            "example@example.com: Valid\n",
            "user@domain: Invalid\n"
          ]
        }
      ]
    }
  ]
}